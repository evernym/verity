######################################################
# Verity Application Reference Config File          #
######################################################

include "verity-version"

akka {
  loglevel = "INFO"
  stdout-loglevel = "INFO"
  # <K8S_SKIP>
  loggers = ["akka.event.slf4j.Slf4jLogger"]
  # <K8S_SKIP>
  logging-filter = "akka.event.slf4j.Slf4jLoggingFilter"

  actor {

    # TODO: this 'java-serialization' is only turned 'on' to allow deserialization of
    # legacy/deprecated 'TransformedMultiEvents/TransformedEvent' (java serialized event)
    # once actor state cleanup (thread context migration etc) is done,
    # we can/should remove this config
    # <K8S_SKIP>
    allow-java-serialization = on

    debug {
      # enable function of Actor.loggable(), which is to log any received message
      # at DEBUG level, see the “Testing Actor Systems” section of the Akka
      # Documentation at http://akka.io/docs
      receive = on

      # enable DEBUG logging of subscription changes on the eventStream
      event-stream = off

      # enable DEBUG logging of unhandled messages
      unhandled = on

      # enable WARN logging of misconfigured routers
      router-misconfiguration = on

    }

    # <K8S_SKIP>
    serializers {
      protoser = "com.evernym.verity.actor.serializers.ProtoBufSerializer"
      kryo-akka = "com.twitter.chill.akka.AkkaSerializer"
    }

    # <K8S_SKIP>
    serialization-bindings {
      "com.evernym.verity.actor.DeprecatedEventMsg" = protoser
      "com.evernym.verity.actor.DeprecatedStateMsg" = protoser
      "com.evernym.verity.actor.DeprecatedMultiEventMsg" = protoser

      "com.evernym.verity.actor.PersistentMsg" = protoser
      "com.evernym.verity.actor.PersistentMultiEventMsg" = protoser

      "com.evernym.verity.actor.ActorMessage" = kryo-akka
    }

    # <K8S_SKIP>
    dispatchers {
      async-op-dispatcher {
        type = Dispatcher
        executor = "thread-pool-executor"
        thread-pool-executor {
          fixed-pool-size = 64
        }
        throughput = 1
      }
    }
    # <K8S_SKIP>
    provider = "akka.cluster.ClusterActorRefProvider"
  }

  cluster {
    sharding {
      passivation.strategy = None
      updating-state-timeout = 25s
    }
    auto-down-unreachable-after = off
    unreachable-nodes-reaper-interval = 10s
    downing-provider-class = "akka.cluster.sbr.SplitBrainResolverProvider"
    split-brain-resolver {
      active-strategy = keep-majority
    } 
  }

  discovery {
    kubernetes-api {
      pod-label-selector = "app=%s" # same as the default
    }
  }

  management {
    http {
      enabled = false
      port = 8558
    }

    cluster.bootstrap {
      enabled = false
      contact-point-discovery {
        service-name = "verity"
        discovery-method = kubernetes-api
      }
    }
  }

  http {
    client {
      parsing {
        max-content-length = 17m
      }
    }

    server {
      remote-address-header = on
      parsing {
        max-uri-length = 67k // 64k (for large invite URLs) + 2k buffer
        max-content-length = 17m
      }
    }

    parsing {
      illegal-header-warnings = off
    }

    host-connection-pool {
      max-connections = 64
      max-open-requests = 256
    }
  }

  persistence {
    journal {
      plugin = "akka.persistence.journal.leveldb"
    }
    snapshot-store {
      plugin = "akka.persistence.snapshot-store.local"
    }
  }

  remote {
    log-remote-lifecycle-events = off


    # Changing threshold to 12 as recommended in the docs. http://doc.akka.io/docs/akka/current/scala/remoting.html#Failure_Detector
    watch-failure-detector.threshold = 12.0

    artery {
      log-sent-messages = off
      canonical {
        # put IP address which other cluster member can reach to (REVIEW_AND_DECIDE)
        hostname = "<getHostAddress>"
      }
      advanced.maximum-frame-size = 4 MiB
    }


  }

  # this is used to know "Legacy" region actor names for "user-agent" and "user-agent-pairwise" actors
  sharding-region-name {
    user-agent = "VerityAgent"
    user-agent-pairwise = "VerityAgentPairwise"
  }

  extensions = [
    "com.evernym.verity.actor.node_singleton.ResourceWarningStatusMngrCache",
    "com.evernym.verity.actor.node_singleton.ResourceBlockingStatusMngrCache",
    "com.evernym.verity.actor.node_singleton.MsgProgressTrackerCache",
    "com.evernym.verity.actor.appStateManager.AppStateUpdateAPI",
    "com.evernym.verity.observability.metrics.MetricsWriterExtension",
    "com.evernym.verity.actor.resourceusagethrottling.helper.ResourceUsageRuleHelperExtension"
  ]

  # find other coordinated shutdown related configurations
  # at path "akka.coordinated-shutdown" at below given link:
  # https://doc.akka.io/docs/akka/current/general/configuration-reference.html#akka-actor
  coordinated-shutdown.phases {
    before-service-unbind {
      //the draining period before coordinated shutdown continues with service-unbinding phase
      // this should be accordence to the frequency of the
      // "heartbeat api (legacy)" or "readiness api (new)"
      // with combination of verity.draining configuration
      timeout = 90 s
    }
  }

  # verity specific kafka configuration (TODO: to be enabled at later/final stage of event bus integration)
  kafka {

    # https://github.com/akka/alpakka-kafka/blob/v3.0.0/core/src/main/resources/reference.conf#L50
    consumer {

      # https://github.com/akka/alpakka-kafka/blob/v3.0.0/core/src/main/resources/reference.conf#L99
      kafka-clients {
        # Required connection configs for Kafka producer, consumer, and admin
        # <K8S-SKIP>
        bootstrap.servers=${verity.kafka.common.bootstrap-servers}
        security.protocol=SASL_SSL
        # <K8S-SKIP>
        sasl.jaas.config=${verity.kafka.common.sasl.jaas.config}
        sasl.mechanism=PLAIN
        # Required for correctness in Apache Kafka clients prior to 2.6
        client.dns.lookup=use_all_dns_ips

        # Best practice for higher availability in Apache Kafka clients prior to 3.0
        session.timeout.ms=45000

        auto.offset.reset: "earliest"

        client.id = ${verity.kafka.common.client.id}
        group.id = ${verity.kafka.consumer.group.id}
      }

      # override verity specific consumer configurations
      stop-timeout = 5 seconds

    }

    # https://github.com/akka/alpakka-kafka/blob/v3.0.0/core/src/main/resources/reference.conf#L165-L190
    # override verity specific configurations
    committer = {
      # Maximum number of messages in a single commit batch
      max-batch = 10
    }

    # https://github.com/akka/alpakka-kafka/blob/v3.0.0/core/src/main/resources/reference.conf#L6
    producer = {

      discovery-method = akka.discovery

      kafka-clients {
        # Required connection configs for Kafka producer, consumer, and admin
        # <K8S-SKIP>
        bootstrap.servers=${verity.kafka.common.bootstrap-servers}
        security.protocol=SASL_SSL
        # <K8S-SKIP>
        sasl.jaas.config=${verity.kafka.common.sasl.jaas.config}
        sasl.mechanism=PLAIN
        # Required for correctness in Apache Kafka clients prior to 2.6
        client.dns.lookup=use_all_dns_ips

        # Best practice for Kafka producer to prevent data loss
        acks=all

        client.id = ${verity.kafka.common.client.id}
      }

      resolve-timeout = 3 seconds
      parallelism = 100
    }
  }
}

alpakka.s3 {
  buffer = "memory"

  aws {
    credentials {
      provider = static
      access-key-id = ""
      secret-access-key = ""
    }

    region {
      provider = static
      default-region = "us-west-2"
    }
  }

  # <K8S_SKIP>
  endpoint-url = null
}

verity {
  agent {
    authentication {

      # determines if this feature is by default available or not
      enabled: false

      # map of 'domain-id' and their authorized keys
      # <K8S_SKIP>
      keys {
        # provided keys will be available to agent belonging to given DID as key
        # examples:
        # domain-id-1: ["key1", "key2"]
        # domain-id-2: ["key3", "key4"]
      }
    }

    # <K8S-SKIP>
    actor-state-cleanup {

      # determines if actor state cleanup (fixing agent route and thread context migration)
      # is enabled or not
      # default value is false until we push a final change to make the whole flow working.
      enabled = false

      # there is a 'ActorStateCleanupManager' actor (as a child of cluster singleton actor)
      # which uses below 'manager' configuration to decide it's behaviour.
      manager {

        # how many parallel 'agent route store' actors will be asked for registeration
        registration {
          # (not hot-reloadable)
          batch-size = 20

          # this is to prevent hitting dynamodb too hard and impact the running service
	        # (not hot-reloadable)
          batch-item-sleep-interval-in-millis = 300
        }

        # how many max 'ActorStateCleanupExecutor' actor would be asked to start processing
        processor {
          # (not hot-reloadable)
          batch-size = 1

          # this is to prevent hitting dynamodb too hard and impact the running service
          # (not hot-reloadable)
          batch-item-sleep-interval-in-millis = 500
        }

        # scheduled job to orchestrating the processing
        scheduled-job {
          # (not hot-reloadable)
          interval-in-seconds = 120
        }
      }

      # there is a 'ActorStateCleanupExecutor' actor (sharded actor, per one 'agent route store' actor)
      # which uses below 'executor' configuration to decide it's behaviour.
      executor {
        # how many max 'routees/agent-actors' would be processed parallely
        # (not hot-reloadable)
        batch-size = 3

        # scheduled job to orchestrating the processing
        scheduled-job {
          # (not hot-reloadable)
          interval-in-seconds = 45
        }
      }
    }

    # <K8S-SKIP>
    migrate-thread-contexts {
      # (hot-reloadable)
      enabled = false

      # (not hot-reloadable)
      batch-size = 20
  
      # (not hot-reloadable)
      batch-item-sleep-interval-in-millis = 0

      scheduled-job {
        # (not hot-reloadable)
        interval-in-seconds = 120
      }
    }

    state.messages {
      get-msgs.limit = 920000

      # message retention/cleanup configuration
      cleanup {
        # if turned on, it will apply message retention logic
        # and accordingly remove messages from state in UserAgent and UserAgentPairwise actor
        # and if snapshot is enabled then in next snapshot those 'removed' messages won't be saved
        # and then next time when actor will recover from snapshot it will be without those 'removed' messages
        # (not hot reloadable for already spinnedup agent actors)
        enabled = true

        # how many days delivered messages should be retained in agent state
        # (not hot reloadable for already spinnedup agent actors)
        days-to-retain-delivered-msgs = 14

        # how many total messages to retain in agent state (non delivered will take priority in this list)
        # (not hot reloadable for already spinnedup agent actors)
        total-msgs-to-retain = 250
      }
    }

    endorser {
        # default endorser did
        did = ""
    }
  }

  app-state-manager {
    state {
      initializing {
        max-retry-count = 10
        max-retry-duration = 240
      }
    }
  }

  # General available blob storage
  blob-store {
    # Path to StorageAPI class to be used. Currently there is a LeveldbAPI and AlpakkaS3API
    storage-service = "com.evernym.verity.storage_services.leveldb.LeveldbAPI"

    # The bucket name will contain <env> depending on which environment is used -> "verity-<env>-blob-storage"
    bucket-name = "blob-store"

    # Optional field for the path to the local db store
    local-store-path = "/tmp/verity/leveldb"
  }

  endorsement.request.txn-store {
    # The bucket name will contain <env> depending on which environment is used -> "verity-<env>-endorsement-request-txn-storage"
    bucket-name = "endorsement-txn"
  }

  cache {
    key-value-mapper {
      expiration-time-in-seconds = 300
    }

    agent-config {
      expiration-time-in-seconds = 300
    }

    agency-detail {
      expiration-time-in-seconds = 1800
    }

    ledger-get-endpoint {
      expiration-time-in-seconds = 300
    }

    ledger-get-schema {
      expiration-time-in-seconds = 1800
    }

    ledger-get-cred-def {
      expiration-time-in-seconds = 1800
    }

    ledger-get-ver-key {
      expiration-time-in-seconds = 1800
    }

    routing-detail {
      expiration-time-in-seconds = 1800
    }

    wallet-get-ver-key {
      expiration-time-in-seconds = 1800
    }
  }

  config {

    # explict place to define paths to skip when generating the k8s helm template. This is
    # useful when in inline comment skip (<K8S_SKIP>) don't work. For example, resolved references
    # don't contain the origional comments to check.
    # <K8S_SKIP>
    k8s-skips = [
      "verity.dynamodb-journal.aws-access-key-id",
      "verity.dynamodb-journal.aws-secret-access-key",
      "verity.dynamodb-journal.endpoint",
      "verity.dynamodb-journal.journal-name",
      "verity.dynamodb-journal.tracing",
      "verity.dynamodb-snapshot-store.aws-access-key-id",
      "verity.dynamodb-snapshot-store.aws-secret-access-key",
      "verity.dynamodb-snapshot-store.endpoint",
      "verity.dynamodb-snapshot-store.journal-name",
      "verity.dynamodb-snapshot-store.tracing",
    ]

    # explict place to define paths that are not in the reference.conf but are should be configurable via
    # k8s helm template. Thses paths SHOULD NOT be in this file but will be added to the template.
    # <K8S_SKIP>
    k8s-includes = [
      "verity.salt.wallet-name",
      "verity.salt.wallet-encryption",
      "verity.salt.event-encryption",

      "verity.secret.routing-agent",
      "verity.secret.token-to-actor-item-mapper-actor",
      "verity.secret.url-mapper-actor",
      "verity.secret.key-value-mapper-actor",
      "verity.secret.user-warning-status-mngr",
      "verity.secret.user-blocking-status-mngr",
      "verity.secret.resource-usage-tracker",
    ]
  }

  dynamodb-common {
    # The service endpoint to connect to for the DynamoDB instance that
    # shall be used. Please refer to the AWS documentation for details.
    endpoint = ""
    aws-access-key-id = ""
    aws-secret-access-key = ""
    journal-name = ""

    # If this is set to `on` then every DynamoDB request will be logged
    # at DEBUG level. Caution: this will generate A LOT of output.
    tracing = off

    # AWS API limits - DO NOT CHANGE UNLESS YOU KNOW WHAT YOU ARE DOING
    # <K8S_SKIP>
    aws-api-limits {
      max-batch-get = 100
      max-batch-write = 25
      max-item-size = 400000
    }

    # AWS client configuration settings, see
    # http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/?com/amazonaws/ClientConfiguration.html
    #
    # (setting any of these to the string "default" means that the corresponding
    # setter method on the ClientConfiguration will not be invoked)
    # <K8S_SKIP>
    aws-client-config {
      client-execution-timeout = default     # int
      connection-max-idle-millis = default   # long
      connection-timeout = default           # int
      connection-ttl = default               # long
      local-address = default                # InetAddress
      max-connections = default              # int
      max-error-retry = default              # int
      preemptive-basic-proxy-auth = default  # boolean
      protocol = HTTPS            # HTTP or HTTPS
      proxy-domain = default                 # string
      proxy-host = default                   # string
      proxy-password = default               # string
      proxy-port = default                   # int
      proxy-username = default               # string
      proxy-workstation = default            # string
      request-timeout = default              # int
      response-metadata-cache-size = default # int
      signer-override = default              # string
      socket-buffer-size-hints = default     # [ int, int ] (for send & receive)
      socket-timeout = default               # int
      use-expect-continue = default          # boolean
      use-gzip = default                     # boolean
      use-reaper = default                   # boolean
      use-tcp-keepalive = default            # boolean
      user-agent = default                   # string
    }
    # <K8S_SKIP>
    dispatcher {
      type = Dispatcher
      executor = "fork-join-executor"
      fork-join-executor {
        parallelism-min = 2
        parallelism-max = 8
      }
    }
  }

  dynamodb-journal {
    # The logical journal name, used in the partition keys.
    journal-table = ""

    # The service endpoint to connect to for the DynamoDB instance that devin
    # shall be used. Please refer to the AWS documentation for details.
    # <K8S_SKIP>
    endpoint = ${verity.dynamodb-common.endpoint}

    # The logical journal name, used in the partition keys.
    # <K8S_SKIP>
    journal-name = ${verity.dynamodb-common.journal-name}

    # The AWS key ID to use for connecting to the specified endpoint.
    # <K8S_SKIP>
    aws-access-key-id = ${verity.dynamodb-common.aws-access-key-id}

    # The AWS secret to use in conjuction with the AWS key ID.
    # <K8S_SKIP>
    aws-secret-access-key = ${verity.dynamodb-common.aws-secret-access-key}

    # <K8S_SKIP>
    aws-client-config = ${verity.dynamodb-common.aws-client-config}

    # AWS API limits - DO NOT CHANGE UNLESS YOU KNOW WHAT YOU ARE DOING
    # <K8S_SKIP>
    aws-api-limits = ${verity.dynamodb-common.aws-api-limits}

    # If this is set to `on` then every DynamoDB request will be logged
    # at DEBUG level. Caution: this will generate A LOT of output.
    # <K8S_SKIP>
    tracing = ${verity.dynamodb-common.tracing}

    # The FQCN for the journal plugin implementation - DO NOT CHANGE
    # <K8S_SKIP>
    class = "akka.persistence.dynamodb.journal.DynamoDBJournal"

    # The dispatcher that executes the replay logic for this plugin
    # instance - should not normally need to be changed.
    # <K8S_SKIP>
    replay-dispatcher = "akka.persistence.dispatchers.default-replay-dispatcher"

    # The dispatcher that executes the write logic for this plugin
    # instance - should not normally need to be changed.
    # <K8S_SKIP>
    plugin-dispatcher = "dynamodb-journal.dispatcher"

    # The number of shards to distribute the sequence number items over.
    sequence-shards = 10

    # number of concurrently running replay prefetch operations for a
    # single PersistentActor; this prefetch means that during a replay
    # more events might be retrieved than specified with the `max`
    # parameter, with corresponding usage DynamoDB read units
    replay-parallelism = 10

    # Set to `on` to log the detected configuration at INFO level at plugin startup.
    log-config = off

    # The dispatcher that executes the future combinators needed for
    # transforming the AmazonDynamoDBAsyncClient results (i.e.
    # handling the back-off etc.)
    # <K8S_SKIP>
    client-dispatcher = "dynamodb-journal.dispatcher"
  }

  dynamodb-snapshot-store {
    # <K8S_SKIP>
    class="akka.persistence.dynamodb.snapshot.DynamoDBSnapshotStore"
    # <K8S_SKIP>
    endpoint = ${verity.dynamodb-common.endpoint}
    # <K8S_SKIP>
    client-dispatcher = "dynamodb-journal.dispatcher"
    snapshot-table = ""
    # <K8S_SKIP>
    journal-name = ${verity.dynamodb-common.journal-name}
    # <K8S_SKIP>
    aws-access-key-id = ${verity.dynamodb-common.aws-access-key-id}
    # <K8S_SKIP>
    aws-secret-access-key = ${verity.dynamodb-common.aws-secret-access-key}

    # If this is set to `on` then every DynamoDB request will be logged
    # at DEBUG level. Caution: this will generate A LOT of output.
    tracing = ${verity.dynamodb-common.tracing}

    # <K8S_SKIP>
    aws-client-config =${verity.dynamodb-common.aws-client-config}

    # AWS API limits - DO NOT CHANGE UNLESS YOU KNOW WHAT YOU ARE DOING
    # <K8S_SKIP>
    aws-api-limits = ${verity.dynamodb-common.aws-api-limits}
  }

  endpoint = {
    # verity host
    host = "localhost"
    # verity default port
    port = "80"
    # agent msg rest api url path, keep default value
    # <K8S-SKIP>
    path-prefix = "agency/msg"
  }

  http {
    # the interface to listen on, keep it default unless you have specific requirement to change it
    interface = "0.0.0.0"

    # the port to listen on for http requests, change as per environment/need
    port = 9000

    # the port to listen on for https requests, change as per environment/need
    # you can comment it if you DON'T want to start ssl binding on this server
    # ssl-port = 9443
  }

  internal-api {
    # The verity setup and health related rest api resources (mostly starting with /agency/internal/... etc)
    # will only be callable from the list of "allowed-from-ip-addresses" (CIDR notation) below.
    # Note that one of the health related api resources shows configuration details, which may contain
    # sensitive information.
    #
    # If allowed-from-ip-addresses is undefined or an empty, by default all "site local" (10 dot, 127 dot,
    # and 172 dot addresses) and loopback intefaces (usually 127.0.0.1) will be allowed. If you add even one
    # CIDR address to the list, you must enumerate all addresses. In other words, if you add 1.2.3.4/32 to the
    # list and you also want all site local and loopback addresses to be allowed, you will need to add the
    # site local and loopback CIDR addresses to the list. The defaults are NOT included if the list is not
    # empty.
    # <K8S-SKIP>
    allowed-from-ip-addresses = ["127.0.0.1/32"]

    # this configuration controls if the 'persistent data' internal api
    # (http://<host>/agency/internal/maintenance/persistent-actor/<actor-type-name>/<actor-entity-id>/data)
    # example: /agency/internal/maintenance/persistent-actor/AgencyAgent/12345/data
    # is available or not for use.
    # the default value is 'false' due to security consideration
    # when it is 'true', then, anyone who has access to internal api (non root users)
    # can see persistent data (snapshot and events) of given persistent actor (bypassing data at rest security)
    persistent-data.enabled = false
  }

  # Unmaintained config. Used for https
  #keystore {
  #  # if you are running ssl-binding, then, provide path (relative to classpath) of .keystore file
  #  location = ".keystore"
  #
  #  # keystore file password
  #  password = "some-password"
  #  password = ${?KEYSTORE_PASSWORD}  //environment variable if set, override above value
  #}

  vdr: {

    # default namespace to be used for unqualified indy ledger identifiers
    # dependening on environment this may/will change to point to the ledger used by the environment before supporting multiple ledgers feature
    unqualified-ledger-prefix = "did:indy:sovrin"

    # this is NOT supposed to be changed based on environment
    prefix-mapping-legacy-did-sov = "did:indy:sovrin"

    # <K8S-SKIP>
    ledger-prefix-mappings {
      "did:sov": ${verity.vdr.prefix-mapping-legacy-did-sov}
    }

    ledgers: [
      {
        type = "indy"
        namespaces = ["indy:sovrin"]
        genesis-txn-file-location = "/var/lib/indy/genesis.txt"
      }
    ]
  }

  lib-vdrtools {
    # library dir location which contains files like libvdrtools.so, libmysqlstorage.so etc
    library-dir-location = "/usr/lib"

    flavor = async

    ledger {
      indy {

        # defines where to fine the pool ledger genesis file. This file is used to bootstrap the pool ledger.
        genesis-txn-file-location = "/var/lib/indy/genesis.txt"

        # it helps libvdrtools keeping different pool configurations in different directory
        # whatever name you provide here, a directory with that name gets created under ~/.indy_client/pool/
        # keeping default should also be ok
        pool-name = "default_pool"

        # defines which indy node txn protocol version to be used
        txn-protocol-version = 2

        transaction_author_agreement = {
          # Transaction Author Agreement
          # https://gitlab.com/evernym/verity/vdr-tools/-/blob/main/docs/how-tos/transaction-author-agreement.md
          #
          # When a Domain Ledger has a Transaction Author Agreement (TAA), configure the current version and any known future
          # versions of the TAA in the transaction_author_agreement configuration below. Each entry must be keyed off of the TAA
          # version and must contain:
          #
          #  - 'digest'
          #     A sha256 hash of the "<version><agreement text>" after trimming <version> and  <agreement text> of all leading and
          #     trailing whitespace. For example, sha256(1.0example TAA text) produces:
          #
          #     f8f50b0c2b7cef2d738cdc87a61fc95e789c80b8cfcb925df7367560500964d7
          #
          #  - 'mechanism'
          #    Verity should use 'on_file', but is configurable here in case the policy changes
          #
          #  - 'time-of-acceptance'
          #    Time of acceptance in yyyy-MM-dd format. You must predate future versions of the TAA with a
          #    time-of-acceptance that is on or after the announced go-live day for a future version. The Sovrin Foundation
          #    has proposed 24 hours before or after the go-live day. This will be configurable by the Sovrin Foundation.
          #    Whatever the Sovrin Foundation decides, it will be plus or minus the "TAA acceptance time valid deviations
          #    (secs)" as defined in indy-plenum:
          #    https://github.com/hyperledger/indy-plenum/blob/aae307ab6ea2c1b0dba4282736d5cfcf5accd28e/plenum/config.py#L410-L411
          #
          # There are several ways to get the current TAA version and text:
          #
          # Option 1: Attempt to bootstrap the verity and let it fail. The response JSON will contain the data you need to add
          # in transaction_author_agreement below. TODO: add verity URL used to bootstrap the Verity here
          #
          # Option 2: Use Indy CLI to get/review the version and text for the active TAA on the ledger
          # https://github.com/hyperledger/indy-sdk/blob/master/docs/how-tos/transaction-author-agreement.md#taa-acceptance-workflow
          # 1. Create and/or open a wallet
          # 2. Create and/or connect to the pool ledger. The TAA will be displayed during this process. Running `pool show-taa`
          #    after connecting to the pool is also an option.
          #
          # Option 3: Use vdr-tools API
          # https://gitlab.com/evernym/verity/vdr-tools/-/blob/main/docs/how-tos/transaction-author-agreement.md
          #
          # Example configuration version a version '1.0.0' with text 'TAA for agency-devlab ledger' would produce a
          # sha256(1.0.0TAA for agency-devlab ledger) hash of 3ae97ea501bd26b81c8c63da2c99696608517d6df8599210c7edaa7e2c719d65
          #
          # It is important to note that time-of-acceptance must be a date plus or minus the
          # "TAA acceptance time valid deviations (secs)" as defined in indy-plenum:
          # https://github.com/hyperledger/indy-plenum/blob/aae307ab6ea2c1b0dba4282736d5cfcf5accd28e/plenum/config.py#L410-L411
          #
          # agreements {
          #   "1.0" {
          #     "digest" = "3ae97ea501bd26b81c8c63da2c99696608517d6df8599210c7edaa7e2c719d65"
          #     "mechanism" = "on_file"
          #     "time-of-acceptance" = "2019-11-18"
          #   }
          # }
          # <K8S-SKIP>
          agreements = {}

          #
          # This setting will cause the following to happen:
          # 1. Will get taa(text and version) from ledger on connection.
          # 2. Will require a matching agreement for the version on the ledger
          # 3. Check that digest is correct for the version
          #
          enabled = false
        }

        pool-config {
          # Defines the timeout to open a pool connection with the ledger AND perform any necessary initialization
          # (i.e. detect and handle the Transaction Author Agreement) of  the Indy Ledger pool connection manager instance
          # in Verity. This should be at least extended-timeout (see below) seconds, but will not be enforced, and will
          # default to 60 seconds if not defined.
          connection-manager-open-timeout = 80

          # Defines the pool timeout in seconds
          # See POOL_ACK_TIMEOUT in vdr-tools/libvdrtools/src/domain/pool.rs
          # See vdr-tools/libvdrtools/src/services/pool/pool.rs:
          #   Call to _get_request_handler_with_ledger_status_sent while in PoolState::Initialization state with a PoolEvent::CheckCache event
          #   Call to _get_f while in PoolState::GettingCatchupTarget state with a PoolEvent::CatchupTargetFound event
          #   Call to _get_request_handler_with_ledger_status_sent while in PoolState::Active state with a PoolEvent::Refresh event
          #   Call to _get_f while in PoolState::Active state with a PoolEvent::SendRequest event
          #   possibly more...
          timeout = 20

          # Defines how long (in seconds) to extend a pool connection's timeout after receiving an ACK (reply) from events
          # processed by the connection. If no reply/ack is received in <extended-timeout> seconds, the pool connection is
          # closed/destroyed.
          # See POOL_REPLY_TIMEOUT in vdr-tools/libvdrtools/src/domain/pool.rs
          # See vdr-tools/libvdrtools/src/services/pool/networker.rs
          extended-timeout = 60

          # Defines the max requests serviced by a pool connection before creating a new pool connection (growing the pool)
          # Pool connections handle <conn-limit> requests before the pool connection is destroyed. In other words, pool
          # connections are rolling (consumed) rather than pooling (reused).
          # See MAX_REQ_PER_POOL_CON in vdr-tools/libvdrtools/src/domain/pool.rs
          conn-limit = 5

          # Defines how long ZMQ polls for events on multiple sockets
          # See POOL_CON_ACTIVE_TO in vdr-tools/libvdrtools/src/domain/pool.r            # ZMQ poll timeout. How long should the pool connection poll for events - See
          # vdr-tools/libvdrtools/src/services/pool/pool.rs
          conn-active-timeout = 5
        }
      }
    }

    wallet {
      type = "default"
    }
  }
  # <K8S-SKIP>
  logging.ignore-logger-filter.logger-name-contains = [
    "DynamoDBClient",
    "DynamoDBJournal",
    "FileUtil",
    "LinuxOperatingSystem",
    "LinuxDisks",
    "LinuxFileSystem"
    "Timer"
  ]

  maintenance {
    agent-routes-migrator {

      enabled = true

      scheduled-job {
        interval-in-seconds = 10
      }

      registration {
        batch-size = 1    //how many parallel "legacy agent route store actor" to ask for registration
      }

      processing {
        batch-size = 1    //how many parallel "legacy agent route store actor" to be processed for migration
      }

      routes {
        //NOTE: whatever this batch size would be, that many entry will be stored by legacy agent route store
        // actor in one event, so make sure this batch-size is not so high that it exceeds
        // 400 k limit of dynamodb
        batch-size = 5    //how many parallel routes "per legacy agent route actor" to be migrated
        batch-item-interval-in-millis = 200   //gap between migrating each route
      }

    }
  }

  metrics {
    service-name: "verity-application"
    tags: {
      env = "ephemeral"
      version = ${?verity.version}
      service = ${?verity.metrics.service-name}
      pod_name = "verity"
    }

    enabled = Y

    # Backend implementation used to write metrics
    backend = "com.evernym.verity.observability.metrics.backend.KamonMetricsBackend"

    # the rules for filter execution are simple: any string is accepted by the filter if
    # it matches at least one of the includes patterns and doesn’t match any of the excludes patterns
    writer {
      # list of metric name regex to be excluded (any one match will exclude it)
      exclude = [
        # below are few examples
        # "metricname",
        # "metricname.*"
      ]
    }

    # There is a specific metric used for the start of protocols. 'uses-sponsor' and 'uses-sponsee' are optional tags
    # that specify if the metric should be tagged with the 'sponsorId' and/or the 'sponseeId'.
    # Typically
    #   - vas will have 'uses-sponsee'=true
    #   - cas will have 'uses-sponsor'=true
    protocol {
      tags {
        uses-sponsor = false
        uses-sponsee = false
      }
    }

    # <K8S-SKIP>
    activity-tracking {
      active-user {
        # Type=Duration -> Found at https://www.scala-lang.org/api/2.9.3/scala/concurrent/duration/Duration.html
        # This is a sliding window (+- timestamp) where the token is considered valid by Verity.
        # If expiration happens, requester will need to request a new token from their sponsor
        # Valid Units: `d, day, h, hour, min, minute, s, sec, second, ms, milli, millisecond, Âµs, micro, microsecond, ns, nano, nanosecond`
        # and their pluralized forms (for every but the first mentioned form of each unit, i.e. no "ds", but "days").
        time-windows = []
        # Monthly is not of type Duration so it can't go in the time-windows array. Monthly is handled a little differently in the code
        monthly-window = false
        enabled = false
      }

      active-relationships {
        # Type=Duration -> Found at https://www.scala-lang.org/api/2.9.3/scala/concurrent/duration/Duration.html
        # This is a sliding window (+- timestamp) where the token is considered valid by Verity.
        # If expiration happens, requester will need to request a new token from their sponsor
        # Valid Units: `d, day, h, hour, min, minute, s, sec, second, ms, milli, millisecond, Âµs, micro, microsecond, ns, nano, nanosecond`
        # and their pluralized forms (for every but the first mentioned form of each unit, i.e. no "ds", but "days").
        time-windows = []
        # Monthly is not of type Duration so it can't go in the time-windows array. Monthly is handled a little differently in the code
        monthly-window = false
        enabled = false
      }
    }

    latency-recording {
      histogram { enabled: false }
      span      { enabled: true }
    }
  }

  # Configure max size of incoming messages per message family
  # Size validation is not performed if message family is not specified in config
  # Default implementation checks for the following optional limits:
  # packed-msg-limit: Message limit for packed msg endpoint
  # rest-limit: Message limit for REST endpoint
  messages.limits {
      issue-credential {
          # Correct value is 172k, increase to 182k was done to unblock specific use case
          # at the risk of getting unhandled errors in other corner cases (see VE-3244)
          packed-msg-limit = 182000
          rest-limit = 182000
      }

      committedanswer {
          packed-msg-limit = 170000
          rest-limit = 170000
      }

      present-proof {
        packed-msg-limit = 293600
        rest-limit = 293600
      }
  }

  msg-template {
    # this is invitation url returned to invitation sender to be used for later purposes
    # this url is also being used by app to fetch invitation detail and show to invitee
    sms-msg-template-invite-url = "#{baseUrl}/agency/invite?t=#{token}"

    # this template is used to build sms content which we want to send to invitee
    sms-msg-template-offer-conn-msg = "#{requesterName} would like you to install Connect-Me for greater identity verification: #{appUrlLink}"

    # template by which deeplink urls are built
    sms-offer-template-deeplink-url = "https://connectme.app.link?t=#{token}"

    # <K8S-SKIP>
    agent-specific {
      # provided specific overrides for agents specified by given domainDID

      # example:
      # <domain-id-1>: {
      #   sms-msg-template-offer-conn-msg = "#{requesterName} would like to connect with you. #{appUrlLink}"
      #   sms-offer-template-deeplink-url = "https://masterpass.app.link?t=#{token}"
      # }
    }

  }

  msgs {
    # default value for connection request expiration time in seconds
    # specific enterprise agent then can update its own agent configuration to override this
    conn-req-expiration-time-in-seconds = 300
  }

  non-persistent-actor {
    base {
      # TODO this does not seem to be used?
      passivate-time-in-seconds = 600

      WalletActor {
        passivate-time-in-seconds = 600
      }
    }
  }

  out-of-band {
    service-key-did-format = false
  }

  persistence {

    # This config is used during storing transformed snapshot (protobified, encrypted etc)
    # to determine if it (post transformation) is not exceeding maximum allowed size.
    # For dynamodb tables with local secondary index, the max allowed size (400KB) includes
    # item data in main table + projected attributes defined in local secondary index
    # (see more details: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Limits.html#limits-items)
    # that is the reason the max snapshot size would/should be little less than half
    # (to be on safer side) of what dynamodb max size (400KB) is allowed in general.
    snapshot.max-item-size-in-bytes = 190000

    use-async-for-msg-forward-feature: false
  }

  # this section covers configuration for any persistent actors
  # it might be based on actor categories, entity type/name or entity id.
  persistent-actor {
    warn-recovery-time-milliseconds = 1000

    base {                                     # for base persistent actor category

      # if set to true, persistent actor will use last saved snapshot (if any) during recovery
      # if set to false, persistent actor will ignore any snapshot (if any) during recovery
      recover-from-snapshots = true

      # receive-timeout-seconds = 600          # applicable to all base persistent actor unless overridden specifically

      # example template
      # ActorType {                            # Replace 'ActorType' accordingly (like AgencyAgent, UserAgent etc)
      #   receive-timeout-seconds = 800        # applicable to all entity of type "ActorType" unless overridden specifically
      #   entity-id-1 {                        # application to only "ActorType-entity-id-1" actor
      #     receive-timeout-seconds = 1000
      #   }
      # }

      # TODO: make sure we are ok with corresponding 'state' proto buf message strucutre
      # before enabling the snapshot
      # TODO: finalize and enable configurations in this below section
      # before enabling/uncommenting them
      AgencyAgent {
        snapshot {
          after-n-events = 100
          keep-n-snapshots = 2
          delete-events-on-snapshots = true
        }
        supervisor {
          enabled = true
          backoff {
            strategy = OnFailure
            min-seconds = 1
            max-seconds = 360
            random-factor = 0.2
            max-nr-of-retries = 10
          }
        }
      }
      AgencyAgentPairwise {
        snapshot {
          after-n-events = 100
          keep-n-snapshots = 2
          delete-events-on-snapshots = true
        }
        supervisor {
          enabled = true
          backoff {
            strategy = OnFailure
            min-seconds = 1
            max-seconds = 360
            random-factor = 0.2
            max-nr-of-retries = 10
          }
        }
      }
      UserAgent {
        snapshot {
          after-n-events = 100
          keep-n-snapshots = 2
          delete-events-on-snapshots = true
        }
        supervisor {
          enabled = true
          backoff {
            strategy = OnFailure
            min-seconds = 1
            max-seconds = 360
            random-factor = 0.2
            max-nr-of-retries = 10
          }
        }
      }
      UserAgentPairwise {
        snapshot {
          after-n-events = 100
          keep-n-snapshots = 2
          delete-events-on-snapshots = true
        }
        supervisor {
          enabled = true
          backoff {
            strategy = OnFailure
            min-seconds = 1
            max-seconds = 360
            random-factor = 0.2
            max-nr-of-retries = 10
          }
        }
      }
      ActivityTracker {
        snapshot {
          after-n-events = 100
          keep-n-snapshots = 2
          delete-events-on-snapshots = true
        }
      }
    }

    protocol-container {                     # applicable to all protocol container actors
    # similar structure as shown under 'base' category above
      supervisor {
        enabled = true
        backoff {
          strategy = OnFailure
          min-seconds = 1
          max-seconds = 360
          random-factor = 0.2
          max-nr-of-retries = 10
        }
      }
    }

    # singleton-children {                     # for singleton children actor category
    # similar structure as shown under 'base' category above
    # }
  }

  # This is used to identify anyone who provisions
  provisioning {
    # sponsors:
    # name: The name of the Sponsor who endorses the provisioning .
    # id: An ID which will not change. Keys cannot be used because of rotation possibilities.
    # keys: VerKey used to sign the token. This can be expanded in the future.
    # endpoint: Endpoint where HTTP message will be sent. Sponsor's backend can deliver to the app however they want.
    # active: boolean if sponsor has an active status with Evernym
    # push-msg-overrides: Configuration for the push messages (overrides for default messages), eg:
    #   general-msg-title-template = "Hi #{targetName}"
    #   general-new-msg-body-template = "#{senderName} sent you #{msgType}"
    #   error-resp-msg-body-template = "#{senderName} responded with error (detail: uid -> #{uid}, msg type -> #{msgType})"
    #   questionanswer_1.0_question-new-msg-body-template = "Your connection is asking you a #{msgType}"
    #   ...
    # <K8S-SKIP>
    sponsors = []
    # Boolean -> some use cases will not what to deal with tokens such as veritysdk.
    sponsor-required = false
    # Type=Duration -> Found at https://www.scala-lang.org/api/2.9.3/scala/concurrent/duration/Duration.html
    # This is a sliding window (+- timestamp) where the token is considered valid by Verity.
    # If expiration happens, requester will need to request a new token from their sponsor
    # Valid Units: `d, day, h, hour, min, minute, s, sec, second, ms, milli, millisecond, Âµs, micro, microsecond, ns, nano, nanosecond`
    # and their pluralized forms (for every but the first mentioned form of each unit, i.e. no "ds", but "days").
    token-window = 10 minute
    # Whether provisioning tokens can be re-used
    cache-used-tokens = false
  }

  # <date time>
  #  - days up to 2 years  -> 1-730 days
  #  TODO: +Inf, and 0 which means on completion of the protocol
  retention-policy {
    protocol-state {
      # A retention policy can be set for a specific domain-id. If nothing is provided, the 'default' values will be used.
      # <protoref>: basicmessage, relationship, issue-credential, present-proof, questionanswer, wallet-backup

      # A retention policy can be set as a default for the entire environment.
      # This is overriden if a specific 'domain-id' provides configuration.
      default {
        # For all undefined protorefs, this is the default policy

        undefined-fallback {

          # defines after how many days data will be expired
          expire-after-days = 3 day

          # if set to true, irrespective of 'expire-after-days', data will be deleted
          # once protocol reaches to terminal state
          # if not set, by default it will be considered false
          expire-after-terminal-state = true
        }
        # you can also define default values for specific protocols
        # <protoref> {
        #   expire-after-days = <days>
        #   expire-after-terminal-state = <boolean>
        # }
      }

      #Example domain registry:
      # <domain-id> {
      #   <protoref> {
      #     expire-after-days = <days>
      #     expire-after-terminal-state = <boolean>
      #   }
      #   For all undefined protorefs, this is the default policy
      #   undefined-fallback {
      #     expire-after-days = <days>
      #     expire-after-terminal-state = <boolean>
      #   }
      # }
    }

    outbox-state {
      # A retention policy can be set for a specific domain-id. If nothing is provided, the 'default' values will be used.
      # <protoref>: basicmessage, relationship, issue-credential, present-proof, questionanswer, wallet-backup

      # A retention policy can be set as a default for the entire environment.
      # This is overriden if a specific 'domain-id' provides configuration.
      default {
        # For all undefined protorefs, this is the default policy

        undefined-fallback {

          # for messages with destination within same domain
          intra-domain {
            # defines after how many days data will be expired
            expire-after-days = 1 day

            # if set to true, irrespective of 'expire-after-days', data will be deleted
            # once the message is delivered to all target outboxes.
            # if not set, by default it will be considered false
            expire-after-terminal-state = true
          }

          # for messages with destination outside of the domain
          inter-domain {
            # defines after how many days data will be expired
            expire-after-days = 1 day

            # if set to true, irrespective of 'expire-after-days', data will be deleted
            # once the message is delivered to all target outboxes.
            # if not set, by default it will be considered false
            expire-after-terminal-state = true
          }
        }
        # you can also define default values for specific protocols
        # <protoref> {
        #   expire-after-days = <days>
        #   expire-after-terminal-state = <boolean>
        # }
      }

      #Example domain registry:
      # <domain-id> {
      #   <protoref> {
      #     expire-after-days = <days>
      #     expire-after-terminal-state = <boolean>
      #   }
      #   For all undefined protorefs, this is the default policy
      #   undefined-fallback {
      #     expire-after-days = <days>
      #     expire-after-terminal-state = <boolean>
      #   }
      # }
    }
  }

  rest-api {
    # This enables the REST endpoint ('<server>/api')
    # If this is set to false, http will respond with 501 Not Implemented
    enabled = false
  }

  services {
    push-notif-service = {
      enabled = false
      general-msg-title-template = "Hi #{targetName}"
      general-new-msg-body-template = "#{senderName} sent you #{msgType}"
      error-resp-msg-body-template = "#{senderName} responded with error (detail: uid -> #{uid}, msg type -> #{msgType})"
      default-sender-name = "Remote connection"
      # <K8S-SKIP>
      msg-types-for-alert-push-notif = [
        "credOffer",
        "cred",
        "proofReq",
        "Question",
        "issue-credential/1.0/offer-credential",
        "issue-credential/1.0/issue-credential",
        "present-proof/1.0/request-presentation",
        "committedanswer/1.0/question",
        "questionanswer/1.0/question",
        "unknown"
      ]
      default-logo-url = ""

      fcm {
        provider = "com.evernym.verity.push_notification.FirebasePusher"
        host = "fcm.googleapis.com"
        path = "/fcm/send"
        key = ""
      }
    }

    sms-service {

      external-services {
        preferred-order = []
        open-market {
          user-name = ""
          password = ""
          service-id = ""
          endpoint {
            host = "servicemanager.openmarket.com"
            port = "443"
            path-prefix = "service/v1/invokeService"
          }
        }
        info-bip {
          endpoint {
            # base-url (can be found here post login: https://www.infobip.com/docs/api)
            host = ""
            port = "443"
            path-prefix = "sms/2/text/advanced"
          }
          access-token = ""
          # alphanumeric sender ID length should be between 3 and 11 characters
          # numeric sender ID length should be between 3 and 14 characters
          sender-id = "EVERNYM"
        }
      }
    }

    url-mapper-service {
      msg-template {
        # template by which shortened url is build
        connect-me-mapped-url-template = "https://connectme.app.link/?t=#{token}"
      }
      endpoint {
        host = ""
        port = 80
        path-prefix = "agency/url-mapper"
      }
    }

    url-shortener-service = {
      # Path to URLShortenerAPI class to be used (YOURLSSvc is available if configured)
      selected = ""
      #selected = "com.evernym.verity.urlshortener.YOURLSSvc"
      #selected = "com.evernym.verity.urlshortener.S3ShortenerSvc"

      yourls {
        api-url = ""
        signature = ""
      }

      # s3 shortener configuration
      s3-shortener {
        url-prefix = ""
        bucket-name = ""
        id-length = 8    # length of short id suffix
        retry-count = 3  # number of retries in case of error

        # here we may override configuration for storage-service
        config-overrides {
          aws {
            region {
              default-region = "us-west-2"
            }
          }
        }
      }
    }
  }

  # thread pool configurations
  thread-pools {
    # <K8S-SKIP>
    default-future {
      # if enabled, for all future code, it will use a custom thread pool executor with below mentioned size
      # if disabled, it will use scala provided implicit global execution context
      # size = 2000
    }

    wallet-future {
      # if enabled, for all wallet future code, it will use a custom thread pool executor with below mentioned size
      # if disabled, it will use above 'default-future' execution cotext configuration
      size = 64
    }
  }

  timeout {
    general-actor-ask-timeout-in-seconds = 30
    general-actor-ref-resolve-timeout-in-seconds = 15
  }

  url-mapper-api {
    enabled = false
  }

  user-agent-pairwise-actor {
    scheduled-job {
      interval-in-seconds = 300
    }
  }

  resource-usage-rules {
    usage-rules {
      default = {}
    }
    blacklisted-tokens = []
    whitelisted-tokens = []
  }

  wallet-storage {
    # the maximum number of database connections allowed
    connection-limit = 20
    read-host-ip = ""
    write-host-ip = ""
    host-port = "3306"
    credentials-username = ""
    credentials-password = ""
    db-name = "wallet"
  }

  draining {
    //maximum check attempts to ensure draining state is communicated
    max-check-count = 5

    //how frequently to check if draining state is communicated/known by the LB
    check-interval = 2 s

    //how much time to wait (to serve existing received requests)
    // before letting service-unbind phase to continue
    wait-before-service-unbind = 60 s
  }

  eventing {

    # event-source emits published events
    # the `basic-source` is verity provided basic eventing infrastructure
    # it should be only used for development/poc purposes only (NON production env)
    event-source = "verity.eventing.basic-source"

    # event-sink takes events to be published
    # sink for the events to be produced
    # the `basic-sink` is verity provided basic eventing infrastructure
    # it should be used for development/poc purposes only (NON production env)
    event-sink = "verity.eventing.basic-sink"

    # basic-store provides in memory event storage (TopicMsgStore actor)
    basic-store {

      # the basic store endpoint handler listens on below given port on each node
      # and when it receives events to be published, it sends it to sharded topic actor
      http-listener {
        host = "localhost"
        port = 8900
      }
    }

    basic-source {
      builder-class = "com.evernym.verity.eventing.adapters.basic.consumer.BasicConsumerAdapterBuilder"
      id = "verity"

      topics = ["public.event.ssi.endorsement", "public.event.ssi.endorser"]

      # event consumer's webhook where topic actor will send published events
      http-listener {
        host = "localhost"
        port = 8901
      }
    }

    basic-sink {
      builder-class = "com.evernym.verity.eventing.adapters.basic.producer.BasicProducerAdapterBuilder"
    }

    # kafka implementation of eventing infrastructure

    # the `kafka-source` emits published events
    # it should be only used for development/poc purposes only (NON production env)
    kafka-source {
      builder-class = "com.evernym.verity.actor.KafkaEventConsumerAdapterBuilder"
    }

    # kafka-sink takes events to be published
    kafka-sink {
      builder-class = "com.evernym.verity.actor.agent.KafkaEventProducerAdapterBuilder"
    }
  }

  kafka {
    common {
      bootstrap-servers = ""
      user-name = ""
      password = ""

      # consumer's or producer's logical application name
      # for consumer: https://kafka.apache.org/documentation.html#consumerconfigs_client.id
      # for producer: https://kafka.apache.org/documentation.html#producerconfigs_client.id
      client.id = ""

      # <K8S_SKIP>
      sasl.jaas.config="org.apache.kafka.common.security.plain.PlainLoginModule   required username='"${?verity.kafka.common.user-name}"'   password='"${?verity.kafka.common.password}"';"
    }

    consumer {
      # A unique string that identifies the consumer group this consumer belongs to
      # https://kafka.apache.org/documentation.html#consumerconfigs_group.id
      group.id = ""

      topics = ["public.event.ssi.endorsement", "public.event.ssi.endorser"]

      # how many consumed messages to be processed parallely
      msg-handling-parallelism = 10
    }
  }
}

kamon {
  environment {
    host = ${verity.endpoint.host}
    service = ${verity.metrics.service-name}
    tag = ${verity.metrics.tags}
  }

  # interval at which metric snapshots will be collected.
  metric.tick-interval = 30 second

  modules.jaeger.enabled = false
  modules.status-page.enabled = false
  
  prometheus.embedded-server {
    hostname = 0.0.0.0
    port = 9095
  }

  modules.datadog-agent.enabled = false
  modules.datadog-trace-agent.enabled = false

  akka.ask-pattern-timeout-warning = lightweight

  # filters are used by several Kamon modules to determine whether to include or exclude
  # certain application components from metrics collection and tracing

  # the rules for filter execution are simple: any string is accepted by the filter if
  # it matches at least one of the includes patterns and doesn't match any of the excludes patterns
  # <K8S-SKIP>
  instrumentation {
    akka.filters {

      actors.track {
        includes = []
        excludes = []
      }

      dispatchers {
        includes = ["akka.actor.default-dispatcher"]
        excludes = []
      }

      trace {
        includes = ["**"]
        excludes = []
      }
    }

    logback.mdc {
      # this is needed for datadog to be able to track operation
      span-operation-name-key = "operation"
    }
  }

  # Disable features of kamon-system-metrics module that are enabled by default that we do not want
  # <K8S-SKIP>
  system-metrics {
    # The sigar library is enabled by default. Disable it due to a core dump while loading/using the sigar library.
    # A fatal error has been detected by the Java Runtime Environment:
    #
    #  SIGSEGV (0xb) at pc=0x00007fa04cf4d311, pid=12147, tid=0x00007fa04d146700
    #
    sigar-enabled = false

    host {
      # Disable collecting all host-level metrics in hopes that the sigar library won't be loaded/used and cause the
      # aforementioned (above) SIGSEGV (core dump)
      enabled = no
    }
  }
}

kanela {
  # <K8S-SKIP>
  show-banner = false
}

cinnamon {
  application = ${verity.metrics.service-name}
  # <K8S-SKIP>
  akka {
    actors {
      "protocol" = {
        report-by = group
        includes = ["com.evernym.verity.protocol.container.actor.*"]
        traceable = on
      }
      "agent" = {
        report-by = group
        includes = ["com.evernym.verity.actor.agent.agency.AgencyAgent",
                    "com.evernym.verity.actor.agent.user.UserAgent",
                    "com.evernym.verity.actor.agent.user.UserAgentPairwise",
                    "com.evernym.verity.actor.agent.agency.AgencyAgentPairwise"
                    ]
        traceable = on
      }
      "app-state-manager" = {
        report-by = group
        includes = ["com.evernym.verity.actor.appStateManager.AppStateManager"]
      }
      "wallet" = {
        report-by = group
        includes = ["com.evernym.verity.actor.wallet.WalletActor"]
        traceable = on
      }
      "item-store" = {
        report-by = group
        includes = ["com.evernym.verity.item_store.ItemStore"]
      }
      "default-SMS-sender" = {
        report-by = group
        includes = ["com.evernym.verity.texter.DefaultSMSSender"]
      }
      "async-op-executor" = {
        report-by = group
        includes = ["com.evernym.verity.protocol.engine.AsyncOpExecutorActor"]
      }
      "url-shortener" = {
        report-by = group
        includes = ["com.evernym.verity.urlshortener.DefaultURLShortener"]
      }
      "outgoing-Msg-sender" = {
        report-by = group
        includes = ["com.evernym.verity.actor.agent.msghandler.outgoing.OutgoingMsgSender"]
      }
      "push-notification" = {
        report-by = group
        includes = ["com.evernym.verity.push_notification.Pusher"]
      }
      "router" = {
        report-by = group
        includes = ["com.evernym.verity.actor.agent.msgrouter.Route"]
        traceable = on
      }
      "resource-blocking-status-manager" = {
        report-by = group
        includes = ["com.evernym.verity.actor.cluster_singleton.resourceusagethrottling.blocking.ResourceBlockingStatusMngr"]
      }
      "segmented-state" = {
        report-by = group
        includes = ["com.evernym.verity.actor.segmentedstates.SegmentedStateStore"]
      }
    }
    http.servers {
      "*:*" {
        paths {
          "*" {
            metrics = on
            traceable = on
            status-code-classes = on
          }
        }
      }
    }
    http.clients {
      "*:*" {
        paths {
          "*" {
            metrics = on
          }
        }
      }
    }
  }

  host = ${verity.endpoint.host}
  application = "verity-application"

  chmetrics {

    reporters += datadog-reporter

    datadog-reporter {
      dogstatsd.custom-metrics-prefix = ""

      host = "127.0.0.1"
      port = 8125
      frequency = 30s
    }

    statsd-reporter {
      dogstatsd {
        custom-metrics-prefix = ""
        tags = ${verity.metrics.tags}
      }
    }
  }
  opentracing.span.tags = ${cinnamon.chmetrics.statsd-reporter.dogstatsd.tags}
  opentracing.datadog {
    service.name = ${verity.metrics.service-name}
    trace.global.tags = "env:"${?verity.metrics.tags.env}",version:"${?verity.metrics.tags.version}
  }

  slf4j.mdc {
    log-correlation += opentracing-trace-id
    opentracing-trace-id {
      name = "trace_id"
    }
  }
}

######################################################
# [END] Verity Applicationd Reference Config File    #
######################################################
